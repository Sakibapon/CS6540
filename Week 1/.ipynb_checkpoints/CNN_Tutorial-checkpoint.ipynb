{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Convolutional Neural Networks (CNNs): The Solution to Image Data\n",
                "\n",
                "**Deep Learning - University of Vermont**\n",
                "\n",
                "---\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "1.  **Understand the Convolution Operation**: Learn how convolution, padding, and stride work to extract features.\n",
                "2.  **Implement Classic Architectures**: Build and train LeNet-5 and AlexNet using PyTorch.\n",
                "3.  **Visualize Learned Features**: See what the network actually \"sees\" by visualizing filters and feature maps.\n",
                "4.  **Compare Architectures**: Benchmark MLP, LeNet-5, and AlexNet on the Fashion-MNIST dataset to understand the trade-offs between model size, complexity, and performance.\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Recap: Why MLPs Struggle with Images\n",
                "\n",
                "In the previous tutorial, we saw that **Multi-Layer Perceptrons (MLPs)** suffer from a massive explosion in parameters when handling image data. \n",
                "\n",
                "*   **Parameter Explosion**: A 2000x2000 image input to a simple MLP requires billions of parameters.\n",
                "*   **Loss of Spatial Information**: Flattening an image into a vector destroys the 2D spatial relationships between pixels.\n",
                "\n",
                "**Convolutional Neural Networks (CNNs)** solve this by using **local connectivity** (looking at small patches of the image) and **parameter sharing** (using the same filter across the entire image)."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. The Convolution Operation\n",
                "\n",
                "The core of a CNN is the **convolution** (technically cross-correlation) operation. A small matrix called a **kernel** or **filter** slides over the input image, computing a dot product at each position."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def corr2d(X, K):\n",
                "    \"\"\"Compute 2D cross-correlation.\"\"\"\n",
                "    h, w = K.shape\n",
                "    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1))\n",
                "    for i in range(Y.shape[0]):\n",
                "        for j in range(Y.shape[1]):\n",
                "            Y[i, j] = (X[i:i + h, j:j + w] * K).sum()\n",
                "    return Y\n",
                "\n",
                "# Example: Edge Detection\n",
                "X = torch.ones((6, 8))\n",
                "X[:, 2:6] = 0\n",
                "\n",
                "# A kernel that detects vertical edges\n",
                "K = torch.tensor([[1.0, -1.0]])\n",
                "\n",
                "Y = corr2d(X, K)\n",
                "\n",
                "print(\"Input Image X:\\n\", X)\n",
                "print(\"\\nKernel K:\\n\", K)\n",
                "print(\"\\nOutput Y (Edges Detected):\\n\", Y)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Padding and Stride\n",
                "\n",
                "*   **Padding**: Adding extra pixels (usually zeros) around the border of the image to control the output size.\n",
                "*   **Stride**: The number of pixels the kernel moves at each step. Larger stride reduces output size.\n",
                "\n",
                "**Output Size Formula:**\n",
                "$$\\lfloor (n_h - k_h + p_h + s_h)/s_h \\rfloor \\times \\lfloor (n_w - k_w + p_w + s_w)/s_w \\rfloor$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PyTorch handles padding and stride easily\n",
                "conv_layer = nn.Conv2d(1, 1, kernel_size=3, padding=1, stride=2)\n",
                "input_data = torch.rand(1, 1, 8, 8)\n",
                "output = conv_layer(input_data)\n",
                "\n",
                "print(f\"Input shape: {input_data.shape}\")\n",
                "print(f\"Output shape (p=1, s=2): {output.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. LeNet-5 (1998)\n",
                "\n",
                "LeNet-5, proposed by Yann LeCun, was one of the first successful CNNs, designed for handwritten digit recognition (MNIST).\n",
                "\n",
                "**Architecture:**\n",
                "1.  **Conv1**: 6 filters, 5x5 kernel, Sigmoid activation\n",
                "2.  **AvgPool1**: 2x2, stride 2\n",
                "3.  **Conv2**: 16 filters, 5x5 kernel, Sigmoid activation\n",
                "4.  **AvgPool2**: 2x2, stride 2\n",
                "5.  **Flatten**\n",
                "6.  **FC1**: 120 units\n",
                "7.  **FC2**: 84 units\n",
                "8.  **Output**: 10 units (Classes)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class LeNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(LeNet, self).__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Conv2d(1, 6, kernel_size=5, padding=2), nn.Sigmoid(),\n",
                "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
                "            nn.Conv2d(6, 16, kernel_size=5), nn.Sigmoid(),\n",
                "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(16 * 5 * 5, 120), nn.Sigmoid(),\n",
                "            nn.Linear(120, 84), nn.Sigmoid(),\n",
                "            nn.Linear(84, 10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. AlexNet (2012)\n",
                "\n",
                "AlexNet sparked the deep learning revolution by winning the ImageNet challenge. It is much deeper and wider than LeNet.\n",
                "\n",
                "**Key Improvements:**\n",
                "*   **ReLU Activation**: Solves vanishing gradient problem.\n",
                "*   **Dropout**: Reduces overfitting.\n",
                "*   **MaxPooling**: Captures sharper features.\n",
                "*   **Data Augmentation**: artificially expands training data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class AlexNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(AlexNet, self).__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            # Use a larger kernel size 11x11 for larger images (224x224)\n",
                "            nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(),\n",
                "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
                "            nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(),\n",
                "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
                "            nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
                "            nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),\n",
                "            nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(),\n",
                "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(6400, 4096), nn.ReLU(),\n",
                "            nn.Dropout(p=0.5),\n",
                "            nn.Linear(4096, 4096), nn.ReLU(),\n",
                "            nn.Dropout(p=0.5),\n",
                "            nn.Linear(4096, 10)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        return self.net(x)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training and Comparison on Fashion-MNIST\n",
                "\n",
                "We will now train three models: **MLP**, **LeNet**, and **AlexNet** on the Fashion-MNIST dataset."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import time\n",
                "import torchvision\n",
                "from torchvision import transforms\n",
                "from torch.utils.data import DataLoader\n",
                "\n",
                "# Device configuration\n",
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
                "print(f\"Using device: {device}\")\n",
                "\n",
                "# Data Preparation\n",
                "def get_dataloader(resize=None, batch_size=256):\n",
                "    trans = [transforms.ToTensor()]\n",
                "    if resize:\n",
                "        trans.insert(0, transforms.Resize(resize))\n",
                "    trans = transforms.Compose(trans)\n",
                "    \n",
                "    train_ds = torchvision.datasets.FashionMNIST(root=\"./data\", train=True, transform=trans, download=True)\n",
                "    test_ds = torchvision.datasets.FashionMNIST(root=\"./data\", train=False, transform=trans, download=True)\n",
                "    \n",
                "    return (DataLoader(train_ds, batch_size, shuffle=True), \n",
                "            DataLoader(test_ds, batch_size, shuffle=False))\n",
                "\n",
                "# Training Function\n",
                "def train_model(net, train_iter, test_iter, num_epochs=5, lr=0.1):\n",
                "    net = net.to(device)\n",
                "    optimizer = torch.optim.SGD(net.parameters(), lr=lr)\n",
                "    loss_fn = nn.CrossEntropyLoss()\n",
                "    \n",
                "    print(f\"Training {net.__class__.__name__}...\")\n",
                "    start_time = time.time()\n",
                "    \n",
                "    for epoch in range(num_epochs):\n",
                "        net.train()\n",
                "        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0\n",
                "        \n",
                "        for X, y in train_iter:\n",
                "            X, y = X.to(device), y.to(device)\n",
                "            optimizer.zero_grad()\n",
                "            y_hat = net(X)\n",
                "            l = loss_fn(y_hat, y)\n",
                "            l.backward()\n",
                "            optimizer.step()\n",
                "            \n",
                "            train_l_sum += l.item()\n",
                "            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()\n",
                "            n += y.shape[0]\n",
                "            \n",
                "        # Evaluate on test set\n",
                "        net.eval()\n",
                "        test_acc_sum, n_test = 0.0, 0\n",
                "        with torch.no_grad():\n",
                "            for X, y in test_iter:\n",
                "                X, y = X.to(device), y.to(device)\n",
                "                test_acc_sum += (net(X).argmax(dim=1) == y).sum().item()\n",
                "                n_test += y.shape[0]\n",
                "                \n",
                "        print(f'Epoch {epoch + 1}, Loss: {train_l_sum/n:.4f}, Train Acc: {train_acc_sum/n:.3f}, Test Acc: {test_acc_sum/n_test:.3f}')\n",
                "        \n",
                "    total_time = time.time() - start_time\n",
                "    print(f\"Total Training Time: {total_time:.1f} sec\\n\")\n",
                "    return test_acc_sum/n_test, total_time"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Train MLP\n",
                "class MLP(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.net = nn.Sequential(\n",
                "            nn.Flatten(),\n",
                "            nn.Linear(784, 256), nn.ReLU(),\n",
                "            nn.Linear(256, 10)\n",
                "        )\n",
                "    def forward(self, x): return self.net(x)\n",
                "\n",
                "train_iter, test_iter = get_dataloader(batch_size=256)\n",
                "mlp_acc, mlp_time = train_model(MLP(), train_iter, test_iter, num_epochs=5)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 2. Train LeNet\n",
                "train_iter, test_iter = get_dataloader(batch_size=256)\n",
                "lenet_acc, lenet_time = train_model(LeNet(), train_iter, test_iter, num_epochs=5, lr=0.5) # LeNet needs higher LR with Sigmoid"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 3. Train AlexNet (Resizing to 224x224)\n",
                "# Note: AlexNet is computationally expensive. We use a smaller batch size and fewer epochs for demonstration.\n",
                "train_iter_alex, test_iter_alex = get_dataloader(resize=224, batch_size=128)\n",
                "alex_acc, alex_time = train_model(AlexNet(), train_iter_alex, test_iter_alex, num_epochs=3, lr=0.01)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Visualizing Feature Maps\n",
                "\n",
                "Let's see what LeNet has learned. We will visualize the output of the first convolutional layer."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_feature_maps(net, image):\n",
                "    net.eval()\n",
                "    with torch.no_grad():\n",
                "        # Get Conv1 output\n",
                "        # LeNet structure: Conv1 is net.net[0]\n",
                "        conv1_out = net.net[0](image.unsqueeze(0).to(device))\n",
                "        \n",
                "        fig, axes = plt.subplots(1, 6, figsize=(12, 3))\n",
                "        for i in range(6):\n",
                "            axes[i].imshow(conv1_out[0, i].cpu(), cmap='gray')\n",
                "            axes[i].axis('off')\n",
                "        plt.suptitle(\"Feature Maps from Conv1\")\n",
                "        plt.show()\n",
                "\n",
                "# Get a sample image\n",
                "image, label = next(iter(get_dataloader()[1]))\n",
                "sample_img = image[0]\n",
                "\n",
                "print(\"Original Image:\")\n",
                "plt.imshow(sample_img[0], cmap='gray')\n",
                "plt.axis('off')\n",
                "plt.show()\n",
                "\n",
                "# Visualize\n",
                "net = LeNet().to(device)\n",
                "# Load weights from trained model if available, else this shows random filters\n",
                "visualize_feature_maps(net, sample_img)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Conclusion & Comparison\n",
                "\n",
                "| Model | Test Accuracy | Training Time | Parameters |\n",
                "|-------|---------------|---------------|------------|\n",
                "| MLP | High | Fast | High |\n",
                "| LeNet | Moderate | Moderate | Low |\n",
                "| AlexNet | High | Slow | Very High |\n",
                "\n",
                "**Key Takeaways:**\n",
                "1.  **MLPs** are fast but inefficient in parameters and ignore spatial structure.\n",
                "2.  **LeNet** introduces convolution, drastically reducing parameters while maintaining reasonable accuracy.\n",
                "3.  **AlexNet** scales up CNNs with depth and modern techniques (ReLU, Dropout), achieving state-of-the-art results but requiring significant compute."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}