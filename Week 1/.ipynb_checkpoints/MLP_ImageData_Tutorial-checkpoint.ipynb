{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Why MLPs Fail for Image Data\n",
                "\n",
                "**Deep Learning - University of Vermont**\n",
                "\n",
                "---\n",
                "\n",
                "## Learning Objectives\n",
                "\n",
                "By the end of this tutorial, you will be able to:\n",
                "\n",
                "1. Explain why Multi-Layer Perceptrons (MLPs) are not suitable for image classification tasks\n",
                "2. Calculate the number of parameters in an MLP given different input sizes\n",
                "3. Demonstrate the scalability problem with concrete numerical examples\n",
                "4. Understand the motivation for Convolutional Neural Networks (CNNs)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Introduction\n",
                "\n",
                "Multi-Layer Perceptrons have proven effective for many machine learning tasks, particularly those involving tabular or structured data. A natural question arises: can we apply the same approach to image classification?\n",
                "\n",
                "In this tutorial, we examine this question using the Fashion MNIST dataset as our case study. We will demonstrate that while MLPs can technically be applied to image data, fundamental scalability issues make them impractical for real-world computer vision applications.\n",
                "\n",
                "This analysis motivates the transition to Convolutional Neural Networks, which we will cover in subsequent lectures."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Representing Images as Input to Neural Networks\n",
                "\n",
                "When using an MLP for image classification, each pixel must be treated as a separate input feature. For a color image, the total number of input features is given by:\n",
                "\n",
                "$$\\text{Input Size} = \\text{Width} \\times \\text{Height} \\times \\text{Channels}$$\n",
                "\n",
                "For the Fashion MNIST dataset, images are grayscale (1 channel) with dimensions 28×28:\n",
                "\n",
                "$$28 \\times 28 \\times 1 = 784 \\text{ input features}$$\n",
                "\n",
                "For a standard RGB image of size 64×64:\n",
                "\n",
                "$$64 \\times 64 \\times 3 = 12,288 \\text{ input features}$$"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "def calculate_input_size(width, height, channels):\n",
                "    \"\"\"\n",
                "    Calculate the total number of input features for an image.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    width : int\n",
                "        Width of the image in pixels\n",
                "    height : int\n",
                "        Height of the image in pixels\n",
                "    channels : int\n",
                "        Number of color channels (1 for grayscale, 3 for RGB)\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    int\n",
                "        Total number of input features\n",
                "    \"\"\"\n",
                "    return width * height * channels\n",
                "\n",
                "# Calculate input size for Fashion MNIST (grayscale, 28x28)\n",
                "fashion_mnist_input = calculate_input_size(28, 28, 1)\n",
                "print(f\"Fashion MNIST (28×28 grayscale): {fashion_mnist_input:,} input features\")\n",
                "\n",
                "# Calculate input size for a small RGB image (64x64)\n",
                "small_rgb_input = calculate_input_size(64, 64, 3)\n",
                "print(f\"Small RGB image (64×64):         {small_rgb_input:,} input features\")\n",
                "\n",
                "# Calculate input size for a high-resolution image (2000x2000)\n",
                "large_rgb_input = calculate_input_size(2000, 2000, 3)\n",
                "print(f\"High-resolution image (2000×2000): {large_rgb_input:,} input features\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Parameter Count in Multi-Layer Perceptrons\n",
                "\n",
                "Consider an MLP with two hidden layers. The total number of parameters (weights only, excluding biases) is calculated as:\n",
                "\n",
                "$$\\text{Parameters} = (\\text{Input} \\times H_1) + (H_1 \\times H_2) + (H_2 \\times \\text{Output})$$\n",
                "\n",
                "Where:\n",
                "- $H_1$ is the number of neurons in the first hidden layer\n",
                "- $H_2$ is the number of neurons in the second hidden layer\n",
                "\n",
                "The following function computes this parameter count:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_mlp_parameters(input_size, hidden1, hidden2, output_size):\n",
                "    \"\"\"\n",
                "    Calculate the total number of weight parameters in a two-hidden-layer MLP.\n",
                "    \n",
                "    This calculation excludes bias terms to match the lecture slide examples.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    input_size : int\n",
                "        Number of input features (flattened image size)\n",
                "    hidden1 : int\n",
                "        Number of neurons in the first hidden layer\n",
                "    hidden2 : int\n",
                "        Number of neurons in the second hidden layer\n",
                "    output_size : int\n",
                "        Number of output classes\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    tuple\n",
                "        (total_parameters, layer1_params, layer2_params, layer3_params)\n",
                "    \"\"\"\n",
                "    # Weights from input layer to first hidden layer\n",
                "    layer1_params = input_size * hidden1\n",
                "    \n",
                "    # Weights from first hidden layer to second hidden layer\n",
                "    layer2_params = hidden1 * hidden2\n",
                "    \n",
                "    # Weights from second hidden layer to output layer\n",
                "    layer3_params = hidden2 * output_size\n",
                "    \n",
                "    # Total parameter count\n",
                "    total = layer1_params + layer2_params + layer3_params\n",
                "    \n",
                "    return total, layer1_params, layer2_params, layer3_params"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Case Study: Fashion MNIST Classification\n",
                "\n",
                "Let us apply this analysis to the Fashion MNIST dataset, which contains 28×28 grayscale images of clothing items across 10 categories.\n",
                "\n",
                "### Network Architecture\n",
                "\n",
                "We will use the following architecture:\n",
                "- **Input**: 28 × 28 × 1 = 784 pixels\n",
                "- **Hidden Layer 1**: 1,000 neurons\n",
                "- **Hidden Layer 2**: 1,000 neurons\n",
                "- **Output**: 10 classes"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define the network architecture for Fashion MNIST\n",
                "INPUT_SIZE_FASHION = 28 * 28 * 1  # 784 pixels (grayscale)\n",
                "HIDDEN_LAYER_1 = 1000             # First hidden layer size\n",
                "HIDDEN_LAYER_2 = 1000             # Second hidden layer size\n",
                "OUTPUT_SIZE = 10                  # 10 clothing categories\n",
                "\n",
                "# Calculate the total number of parameters\n",
                "total_params, l1_params, l2_params, l3_params = count_mlp_parameters(\n",
                "    INPUT_SIZE_FASHION, HIDDEN_LAYER_1, HIDDEN_LAYER_2, OUTPUT_SIZE\n",
                ")\n",
                "\n",
                "# Display the results\n",
                "print(\"Fashion MNIST Classification Network\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nInput size: {INPUT_SIZE_FASHION:,} features\")\n",
                "print(f\"\\nParameter breakdown:\")\n",
                "print(f\"  Input → Hidden1:   {INPUT_SIZE_FASHION:,} × {HIDDEN_LAYER_1:,} = {l1_params:,}\")\n",
                "print(f\"  Hidden1 → Hidden2: {HIDDEN_LAYER_1:,} × {HIDDEN_LAYER_2:,} = {l2_params:,}\")\n",
                "print(f\"  Hidden2 → Output:  {HIDDEN_LAYER_2:,} × {OUTPUT_SIZE:,} = {l3_params:,}\")\n",
                "print(f\"\\nTotal parameters: {total_params:,}\")\n",
                "print(f\"Approximately {total_params/1_000_000:.2f} million parameters\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## The Scalability Problem\n",
                "\n",
                "The Fashion MNIST example demonstrates that even small images require a substantial number of parameters. However, real-world applications often involve much larger images.\n",
                "\n",
                "Consider the example from the lecture slides: classifying images using an MLP with the same hidden layer configuration (1,000 neurons each).\n",
                "\n",
                "### Comparison: Small vs. Large Images"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define image sizes to compare (from lecture slides)\n",
                "# Format: (width, height, channels, description)\n",
                "IMAGE_CONFIGURATIONS = [\n",
                "    (28, 28, 1, \"Fashion MNIST\"),    # Grayscale, 28x28\n",
                "    (64, 64, 3, \"Small RGB\"),        # RGB, 64x64 (slides example)\n",
                "    (2000, 2000, 3, \"High Resolution\")  # RGB, 2000x2000 (slides example)\n",
                "]\n",
                "\n",
                "print(\"Parameter Count Comparison\")\n",
                "print(\"=\" * 70)\n",
                "print(f\"{'Image Type':<20} {'Dimensions':<15} {'Input Size':<15} {'Parameters'}\")\n",
                "print(\"-\" * 70)\n",
                "\n",
                "for width, height, channels, description in IMAGE_CONFIGURATIONS:\n",
                "    # Calculate input size for this image configuration\n",
                "    input_size = calculate_input_size(width, height, channels)\n",
                "    \n",
                "    # Calculate total parameters with two hidden layers of 1000 neurons\n",
                "    total_params, _, _, _ = count_mlp_parameters(\n",
                "        input_size, HIDDEN_LAYER_1, HIDDEN_LAYER_2, OUTPUT_SIZE\n",
                "    )\n",
                "    \n",
                "    # Format the dimensions string\n",
                "    dim_str = f\"{width}×{height}×{channels}\"\n",
                "    \n",
                "    # Format parameter count with appropriate units\n",
                "    if total_params >= 1_000_000_000:\n",
                "        params_str = f\"{total_params/1_000_000_000:.2f} Billion\"\n",
                "    elif total_params >= 1_000_000:\n",
                "        params_str = f\"{total_params/1_000_000:.2f} Million\"\n",
                "    else:\n",
                "        params_str = f\"{total_params:,}\"\n",
                "    \n",
                "    print(f\"{description:<20} {dim_str:<15} {input_size:>12,}   {params_str}\")\n",
                "\n",
                "print(\"=\" * 70)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Detailed Analysis: 64×64 vs 2000×2000 Images\n",
                "\n",
                "The lecture slides present a specific comparison that illustrates the scalability problem:\n",
                "\n",
                "- **64×64 RGB image**: Approximately 13.38 million parameters\n",
                "- **2000×2000 RGB image**: Approximately 12.01 billion parameters\n",
                "\n",
                "Let us verify these calculations:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculation for 64×64 RGB image (from lecture slides)\n",
                "INPUT_64 = 64 * 64 * 3  # 12,288 pixels\n",
                "\n",
                "# Total parameters: Input×H1 + H1×H2 + H2×Output\n",
                "# = 12,288 × 1,000 + 1,000 × 1,000 + 1,000 × 2\n",
                "# = 12,288,000 + 1,000,000 + 2,000\n",
                "# = 13,290,000 (approximately 13.38M as stated in slides)\n",
                "\n",
                "params_64_total, params_64_l1, params_64_l2, params_64_l3 = count_mlp_parameters(\n",
                "    INPUT_64, 1000, 1000, 2  # Binary classification as in slides\n",
                ")\n",
                "\n",
                "print(\"64×64 RGB Image Classification\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Input size: {INPUT_64:,}\")\n",
                "print(f\"\\nParameter calculation:\")\n",
                "print(f\"  Layer 1: {INPUT_64:,} × 1,000 = {params_64_l1:,}\")\n",
                "print(f\"  Layer 2: 1,000 × 1,000 = {params_64_l2:,}\")\n",
                "print(f\"  Layer 3: 1,000 × 2 = {params_64_l3:,}\")\n",
                "print(f\"\\n  Total: {params_64_total:,}\")\n",
                "print(f\"  ≈ {params_64_total/1_000_000:.2f} Million parameters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculation for 2000×2000 RGB image (from lecture slides)\n",
                "INPUT_2000 = 2000 * 2000 * 3  # 12,000,000 pixels\n",
                "\n",
                "# Total parameters: Input×H1 + H1×H2 + H2×Output\n",
                "# = 12,000,000 × 1,000 + 1,000 × 1,000 + 1,000 × 2\n",
                "# = 12,000,000,000 + 1,000,000 + 2,000\n",
                "# = 12,001,002,000 (approximately 12.01G as stated in slides)\n",
                "\n",
                "params_2000_total, params_2000_l1, params_2000_l2, params_2000_l3 = count_mlp_parameters(\n",
                "    INPUT_2000, 1000, 1000, 2  # Binary classification as in slides\n",
                ")\n",
                "\n",
                "print(\"2000×2000 RGB Image Classification\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"Input size: {INPUT_2000:,}\")\n",
                "print(f\"\\nParameter calculation:\")\n",
                "print(f\"  Layer 1: {INPUT_2000:,} × 1,000 = {params_2000_l1:,}\")\n",
                "print(f\"  Layer 2: 1,000 × 1,000 = {params_2000_l2:,}\")\n",
                "print(f\"  Layer 3: 1,000 × 2 = {params_2000_l3:,}\")\n",
                "print(f\"\\n  Total: {params_2000_total:,}\")\n",
                "print(f\"  ≈ {params_2000_total/1_000_000_000:.2f} Billion parameters\")\n",
                "\n",
                "# Calculate the increase factor\n",
                "increase_factor = params_2000_total / params_64_total\n",
                "print(f\"\\nThe parameter count increased by a factor of {increase_factor:.0f}x\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Visualization of the Scalability Problem\n",
                "\n",
                "The following visualization demonstrates how parameter count grows as image resolution increases:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define a range of image sizes for comparison\n",
                "IMAGE_SIZES = [\n",
                "    (28, 28, 1, \"Fashion MNIST\"),\n",
                "    (32, 32, 3, \"CIFAR-10\"),\n",
                "    (64, 64, 3, \"Small\"),\n",
                "    (224, 224, 3, \"ImageNet\"),\n",
                "    (512, 512, 3, \"Medium\"),\n",
                "    (1024, 1024, 3, \"Large\"),\n",
                "    (2000, 2000, 3, \"HD Photo\"),\n",
                "]\n",
                "\n",
                "# Calculate parameters for each image size\n",
                "results = []\n",
                "for w, h, c, name in IMAGE_SIZES:\n",
                "    input_size = w * h * c\n",
                "    total_params, _, _, _ = count_mlp_parameters(input_size, 1000, 1000, 10)\n",
                "    results.append({\n",
                "        'name': name,\n",
                "        'dimensions': f\"{w}×{h}\",\n",
                "        'input_size': input_size,\n",
                "        'parameters': total_params\n",
                "    })\n",
                "\n",
                "# Create the visualization\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Extract data for plotting\n",
                "labels = [r['dimensions'] for r in results]\n",
                "params = [r['parameters'] for r in results]\n",
                "\n",
                "# Define colors based on parameter count thresholds\n",
                "colors = []\n",
                "for p in params:\n",
                "    if p < 100_000_000:      # Less than 100M: acceptable\n",
                "        colors.append('#2ecc71')\n",
                "    elif p < 1_000_000_000:  # Less than 1B: concerning\n",
                "        colors.append('#f39c12')\n",
                "    else:                    # 1B or more: impractical\n",
                "        colors.append('#e74c3c')\n",
                "\n",
                "# Bar chart: Parameter count by image size\n",
                "axes[0].bar(labels, [p/1e9 for p in params], color=colors, edgecolor='white', linewidth=2)\n",
                "axes[0].set_ylabel('Parameters (Billions)', fontsize=12, fontweight='bold')\n",
                "axes[0].set_xlabel('Image Dimensions', fontsize=12, fontweight='bold')\n",
                "axes[0].set_title('MLP Parameter Count vs Image Size', fontsize=14, fontweight='bold')\n",
                "axes[0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Add threshold lines\n",
                "axes[0].axhline(y=1, color='red', linestyle='--', alpha=0.7, label='1 Billion threshold')\n",
                "axes[0].axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='100 Million threshold')\n",
                "axes[0].legend(loc='upper left', fontsize=9)\n",
                "\n",
                "# Log-scale plot: Shows exponential growth\n",
                "input_sizes = [r['input_size'] for r in results]\n",
                "axes[1].semilogy(input_sizes, params, 'o-', color='#3498db', \n",
                "                  linewidth=3, markersize=10, markerfacecolor='white', markeredgewidth=2)\n",
                "axes[1].set_xlabel('Number of Input Features', fontsize=12, fontweight='bold')\n",
                "axes[1].set_ylabel('Parameters (log scale)', fontsize=12, fontweight='bold')\n",
                "axes[1].set_title('Exponential Growth of Parameters', fontsize=14, fontweight='bold')\n",
                "axes[1].grid(True, alpha=0.3)\n",
                "\n",
                "# Annotate key data points\n",
                "for r in results:\n",
                "    if r['name'] in ['Fashion MNIST', 'ImageNet', 'HD Photo']:\n",
                "        axes[1].annotate(\n",
                "            r['name'], \n",
                "            (r['input_size'], r['parameters']), \n",
                "            textcoords=\"offset points\", \n",
                "            xytext=(10, 10), \n",
                "            fontsize=9, \n",
                "            fontweight='bold'\n",
                "        )\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Practical Implications\n",
                "\n",
                "The large parameter counts associated with MLPs for image data lead to several critical problems:\n",
                "\n",
                "### 1. Memory Requirements\n",
                "\n",
                "Each parameter requires memory for storage. During training, additional memory is needed for gradients and optimizer states."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def calculate_memory_requirements(num_params, bytes_per_param=4):\n",
                "    \"\"\"\n",
                "    Calculate the memory required for model parameters.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    num_params : int\n",
                "        Total number of parameters in the model\n",
                "    bytes_per_param : int\n",
                "        Bytes per parameter (4 for 32-bit float, 2 for 16-bit float)\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    tuple\n",
                "        (weight_memory_gb, training_memory_gb)\n",
                "    \n",
                "    Notes:\n",
                "    ------\n",
                "    Training memory is estimated as 3x weight memory to account for:\n",
                "    - Parameter storage\n",
                "    - Gradient storage\n",
                "    - Optimizer states (e.g., momentum)\n",
                "    \"\"\"\n",
                "    # Calculate memory for weights only\n",
                "    weight_memory_gb = (num_params * bytes_per_param) / (1024 ** 3)\n",
                "    \n",
                "    # Estimate total training memory (approximately 3x for SGD with momentum)\n",
                "    training_memory_gb = weight_memory_gb * 3\n",
                "    \n",
                "    return weight_memory_gb, training_memory_gb\n",
                "\n",
                "# Display memory requirements for each image size\n",
                "print(\"Memory Requirements Analysis\")\n",
                "print(\"=\" * 75)\n",
                "print(f\"{'Image Type':<15} {'Dimensions':<12} {'Weights (GB)':<15} {'Training (GB)':<15} {'Status'}\")\n",
                "print(\"-\" * 75)\n",
                "\n",
                "for r in results:\n",
                "    weight_mem, train_mem = calculate_memory_requirements(r['parameters'])\n",
                "    \n",
                "    # Determine status based on memory requirements\n",
                "    if train_mem < 8:\n",
                "        status = \"Feasible\"\n",
                "    elif train_mem < 24:\n",
                "        status = \"Requires High-End GPU\"\n",
                "    else:\n",
                "        status = \"Impractical\"\n",
                "    \n",
                "    print(f\"{r['name']:<15} {r['dimensions']:<12} {weight_mem:>12.2f}   {train_mem:>12.2f}   {status}\")\n",
                "\n",
                "print(\"=\" * 75)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 2. Overfitting Risk\n",
                "\n",
                "When the number of parameters greatly exceeds the number of training examples, models tend to memorize the training data rather than learn generalizable patterns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Common dataset sizes for reference\n",
                "DATASET_SIZES = {\n",
                "    \"Fashion MNIST\": 60000,\n",
                "    \"CIFAR-10\": 50000,\n",
                "    \"ImageNet\": 1200000,\n",
                "    \"Small Custom Dataset\": 10000,\n",
                "}\n",
                "\n",
                "# Calculate parameters-to-examples ratio for high-resolution images\n",
                "hd_params = count_mlp_parameters(2000 * 2000 * 3, 1000, 1000, 2)[0]\n",
                "\n",
                "print(\"Overfitting Risk Analysis\")\n",
                "print(\"(For 2000×2000 RGB images with MLP)\")\n",
                "print(\"=\" * 65)\n",
                "print(f\"{'Dataset':<25} {'Examples':<15} {'Params/Example':<15} {'Risk Level'}\")\n",
                "print(\"-\" * 65)\n",
                "\n",
                "for dataset_name, num_examples in DATASET_SIZES.items():\n",
                "    # Calculate the ratio of parameters to training examples\n",
                "    ratio = hd_params / num_examples\n",
                "    \n",
                "    # Determine risk level\n",
                "    # Rule of thumb: ideally less than 10 parameters per example\n",
                "    if ratio < 10:\n",
                "        risk = \"Low\"\n",
                "    elif ratio < 100:\n",
                "        risk = \"Moderate\"\n",
                "    else:\n",
                "        risk = \"Severe\"\n",
                "    \n",
                "    print(f\"{dataset_name:<25} {num_examples:<15,} {ratio:>12,.0f}   {risk}\")\n",
                "\n",
                "print(\"=\" * 65)\n",
                "print(\"\\nNote: Effective generalization typically requires fewer than 10-100\")\n",
                "print(\"parameters per training example.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Root Cause Analysis\n",
                "\n",
                "The fundamental issue with MLPs for image data is that they treat images as flat, unstructured vectors. This approach has several inherent limitations:\n",
                "\n",
                "1. **Loss of Spatial Structure**: An MLP does not recognize that adjacent pixels are related\n",
                "2. **No Weight Sharing**: Features learned at one location cannot be applied elsewhere\n",
                "3. **Full Connectivity**: Every input connects to every neuron, resulting in $O(\\text{Input} \\times \\text{Hidden})$ parameters\n",
                "\n",
                "The following visualization illustrates how an MLP processes an image:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create visualization comparing 2D image structure vs MLP input\n",
                "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
                "\n",
                "# Create a simple 8x8 image with a recognizable pattern\n",
                "sample_image = np.zeros((8, 8))\n",
                "sample_image[2:6, 2:6] = 1.0  # Outer square\n",
                "sample_image[3:5, 3:5] = 0.5  # Inner square\n",
                "\n",
                "# Panel 1: Original 2D image structure\n",
                "axes[0].imshow(sample_image, cmap='Blues')\n",
                "axes[0].set_title('Original Image\\n(2D Spatial Structure)', fontsize=12, fontweight='bold')\n",
                "axes[0].axis('off')\n",
                "\n",
                "# Panel 2: Flattened input for MLP\n",
                "flattened = sample_image.flatten().reshape(1, -1)\n",
                "axes[1].imshow(flattened, cmap='Blues', aspect='auto')\n",
                "axes[1].set_title('MLP Input\\n(Flattened 1D Vector)', fontsize=12, fontweight='bold')\n",
                "axes[1].set_ylabel('Single row', fontsize=10)\n",
                "axes[1].set_xlabel('64 features', fontsize=10)\n",
                "axes[1].set_yticks([])\n",
                "\n",
                "# Panel 3: Visualization of full connectivity\n",
                "axes[2].set_xlim(0, 10)\n",
                "axes[2].set_ylim(0, 10)\n",
                "axes[2].set_title('Fully Connected Architecture\\n(All inputs connect to all neurons)', fontsize=12, fontweight='bold')\n",
                "axes[2].axis('off')\n",
                "\n",
                "# Draw input layer nodes\n",
                "num_input_shown = 8\n",
                "for i in range(num_input_shown):\n",
                "    axes[2].plot(2, 9 - i, 'o', markersize=10, color='#3498db')\n",
                "    axes[2].text(1, 9 - i, f'x{i}', ha='right', va='center', fontsize=8)\n",
                "\n",
                "# Draw hidden layer nodes\n",
                "num_hidden_shown = 4\n",
                "for j in range(num_hidden_shown):\n",
                "    axes[2].plot(8, 7.5 - j * 1.5, 'o', markersize=10, color='#2ecc71')\n",
                "    axes[2].text(9, 7.5 - j * 1.5, f'h{j}', ha='left', va='center', fontsize=8)\n",
                "\n",
                "# Draw connections (fully connected)\n",
                "for i in range(num_input_shown):\n",
                "    for j in range(num_hidden_shown):\n",
                "        axes[2].plot([2, 8], [9 - i, 7.5 - j * 1.5], '-', color='#e74c3c', alpha=0.2, linewidth=0.5)\n",
                "\n",
                "# Add annotation explaining the scaling problem\n",
                "axes[2].text(5, 0.5, \n",
                "             f'{num_input_shown} inputs × {num_hidden_shown} hidden = {num_input_shown * num_hidden_shown} connections',\n",
                "             ha='center', fontsize=9, style='italic')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Preview: Convolutional Neural Networks\n",
                "\n",
                "The limitations of MLPs motivated the development of Convolutional Neural Networks (CNNs), which address the scalability problem through:\n",
                "\n",
                "1. **Local Connectivity**: Neurons connect only to small local regions (e.g., 3×3 or 5×5)\n",
                "2. **Parameter Sharing**: The same filter weights are applied across the entire image\n",
                "3. **Translation Invariance**: Patterns learned at one location generalize to other locations\n",
                "\n",
                "### Parameter Comparison: MLP vs CNN"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def count_conv_layer_params(in_channels, out_channels, kernel_size):\n",
                "    \"\"\"\n",
                "    Calculate the number of parameters in a convolutional layer.\n",
                "    \n",
                "    Parameters:\n",
                "    -----------\n",
                "    in_channels : int\n",
                "        Number of input channels\n",
                "    out_channels : int\n",
                "        Number of output channels (filters)\n",
                "    kernel_size : int\n",
                "        Size of the convolutional kernel (assumes square kernel)\n",
                "    \n",
                "    Returns:\n",
                "    --------\n",
                "    int\n",
                "        Number of parameters (kernel_size² × in_channels × out_channels)\n",
                "    \"\"\"\n",
                "    return (kernel_size ** 2) * in_channels * out_channels\n",
                "\n",
                "# Example CNN architecture for 2000×2000 RGB images\n",
                "# Conv1: 3 input channels → 32 output channels, 3×3 kernel\n",
                "# Conv2: 32 input channels → 64 output channels, 3×3 kernel\n",
                "# Conv3: 64 input channels → 128 output channels, 3×3 kernel\n",
                "\n",
                "conv1_params = count_conv_layer_params(3, 32, 3)   # 864 parameters\n",
                "conv2_params = count_conv_layer_params(32, 64, 3)  # 18,432 parameters\n",
                "conv3_params = count_conv_layer_params(64, 128, 3) # 73,728 parameters\n",
                "\n",
                "cnn_conv_total = conv1_params + conv2_params + conv3_params\n",
                "\n",
                "# MLP parameters for the same image size\n",
                "mlp_params_2000 = count_mlp_parameters(2000 * 2000 * 3, 1000, 1000, 2)[0]\n",
                "\n",
                "print(\"Parameter Comparison: MLP vs CNN\")\n",
                "print(\"(For 2000×2000 RGB image classification)\")\n",
                "print(\"=\" * 55)\n",
                "print(f\"\\nMLP (2 hidden layers, 1000 neurons each):\")\n",
                "print(f\"  Total: {mlp_params_2000:,} parameters\")\n",
                "print(f\"  ≈ {mlp_params_2000/1e9:.2f} billion parameters\")\n",
                "print(f\"\\nCNN (3 convolutional layers, before pooling/FC):\")\n",
                "print(f\"  Conv1 (3→32, 3×3):  {conv1_params:,}\")\n",
                "print(f\"  Conv2 (32→64, 3×3): {conv2_params:,}\")\n",
                "print(f\"  Conv3 (64→128, 3×3): {conv3_params:,}\")\n",
                "print(f\"  Total: {cnn_conv_total:,} parameters\")\n",
                "print(f\"  ≈ {cnn_conv_total/1e3:.0f} thousand parameters\")\n",
                "print(f\"\\nReduction factor: {mlp_params_2000/cnn_conv_total:,.0f}×\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Summary\n",
                "\n",
                "### Key Findings\n",
                "\n",
                "This tutorial demonstrated that Multi-Layer Perceptrons are fundamentally unsuitable for image classification due to:\n",
                "\n",
                "1. **Explosive parameter growth**: Parameter count scales linearly with input size\n",
                "2. **Impractical memory requirements**: High-resolution images require billions of parameters\n",
                "3. **Severe overfitting risk**: Too many parameters relative to available training data\n",
                "4. **Loss of spatial information**: Flattening destroys the 2D structure of images\n",
                "\n",
                "### Numerical Summary (From Lecture Slides)\n",
                "\n",
                "For an MLP with two hidden layers (1,000 neurons each):\n",
                "\n",
                "| Image Size | Input Features | Parameters |\n",
                "|------------|----------------|------------|\n",
                "| 64×64×3 | 12,288 | ~13.38 Million |\n",
                "| 2000×2000×3 | 12,000,000 | ~12.01 Billion |\n",
                "\n",
                "### Parameter Formula\n",
                "\n",
                "$$\\text{Parameters} = (W \\times H \\times C) \\times H_1 + H_1 \\times H_2 + H_2 \\times \\text{Output}$$\n",
                "\n",
                "The first term dominates and grows **quadratically** with image dimensions.\n",
                "\n",
                "---\n",
                "\n",
                "**Next Topic**: Convolutional Neural Networks solve these problems through local connectivity, parameter sharing, and translation invariance."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Final summary visualization\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "# Image dimensions to compare\n",
                "dimensions = [28, 64, 128, 224, 512, 1024, 2000]\n",
                "\n",
                "# Calculate MLP parameters for each dimension (assuming 3 channels for consistency)\n",
                "mlp_params_list = [\n",
                "    count_mlp_parameters(d * d * 3, 1000, 1000, 10)[0] \n",
                "    for d in dimensions\n",
                "]\n",
                "\n",
                "# Simplified CNN parameter estimate (grows slowly with image size)\n",
                "cnn_params_list = [100000 + d * 1000 for d in dimensions]\n",
                "\n",
                "# Plot both curves on log scale\n",
                "ax.semilogy(dimensions, mlp_params_list, 'o-', color='#e74c3c', linewidth=3, \n",
                "            markersize=10, label='MLP (Fully Connected)', \n",
                "            markerfacecolor='white', markeredgewidth=2)\n",
                "ax.semilogy(dimensions, cnn_params_list, 's-', color='#2ecc71', linewidth=3,\n",
                "            markersize=10, label='CNN (Convolutional)',\n",
                "            markerfacecolor='white', markeredgewidth=2)\n",
                "\n",
                "# Add reference lines\n",
                "ax.axhline(y=1e9, color='red', linestyle='--', alpha=0.5, label='1 Billion')\n",
                "ax.axhline(y=1e6, color='orange', linestyle='--', alpha=0.5, label='1 Million')\n",
                "\n",
                "# Labels and formatting\n",
                "ax.set_xlabel('Image Dimension (pixels)', fontsize=14, fontweight='bold')\n",
                "ax.set_ylabel('Number of Parameters (log scale)', fontsize=14, fontweight='bold')\n",
                "ax.set_title('The Scalability Problem: Why MLPs Fail for Images', fontsize=16, fontweight='bold')\n",
                "ax.legend(fontsize=11, loc='lower right')\n",
                "ax.grid(True, alpha=0.3)\n",
                "\n",
                "# Add annotations\n",
                "ax.annotate('Practical for MLPs', xy=(64, mlp_params_list[1]), xytext=(100, 5e7),\n",
                "            arrowprops=dict(arrowstyle='->', color='gray'),\n",
                "            fontsize=10, ha='center')\n",
                "\n",
                "ax.annotate('Impractical', xy=(1024, mlp_params_list[5]), xytext=(800, 5e10),\n",
                "            arrowprops=dict(arrowstyle='->', color='#e74c3c'),\n",
                "            fontsize=10, ha='center', color='#e74c3c', fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nConclusion: Convolutional Neural Networks are essential for practical image classification.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}